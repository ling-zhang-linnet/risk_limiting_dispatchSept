{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMm4CJAsaGT8tpz43Qw8GuD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhhhling/risk_limiting_dispatchSept/blob/main/118bus_benchmarks_neural_policy_for_rld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q3UWPo6mISo",
        "outputId": "8e28adc0-b1e3-4250-b10b-64ad7c96690a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in case gpu is used\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QCAsDEcmR1O",
        "outputId": "7bfae927-f8fc-4f62-ce8e-ded1cb2a3319"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "import cvxpy as cp\n",
        "import random\n",
        "from numpy import savetxt\n",
        "import argparse\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "root_path = './gdrive/MyDrive/Inbox/rld_neural_policy/'"
      ],
      "metadata": {
        "id": "mvxphpwn1Th-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# helper functions, can be folded"
      ],
      "metadata": {
        "id": "7AfB0oo7mnIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils.py"
      ],
      "metadata": {
        "id": "JRt8IRhLmx8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dir(PATH):\n",
        "    isExist = os.path.exists(PATH)\n",
        "\n",
        "    if not isExist:\n",
        "        # Create a new directory if it does not exist \n",
        "        os.makedirs(PATH)\n",
        "        print(\"The new directory is created!\")\n",
        "\n",
        "\n",
        "# Evaluate using vector distance\n",
        "def measure_relative_distance(v1, v2):\n",
        "    '''\n",
        "        Note that v1 is the benchmark.\n",
        "        Norm is calculated along dimension/axis 1\n",
        "        and average is calculated along dimension/axis 0\n",
        "        Also return the distance vetor.\n",
        "    '''\n",
        "    if len(v1.shape)==1 and len(v2.shape)==1:\n",
        "        distance = np.abs(v1-v2)/np.abs(v1)\n",
        "\n",
        "    if len(v1.shape)==2 and len(v2.shape)==2:\n",
        "        distance = np.linalg.norm(v1-v2, axis=1)/np.linalg.norm(v1, axis=1)\n",
        "\n",
        "    return distance, np.mean(distance)"
      ],
      "metadata": {
        "id": "R4SwyMaKmmou"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## models.py"
      ],
      "metadata": {
        "id": "PXvt9h5Rm_cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionNet(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(ActionNet, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.linear2 = torch.nn.Linear(H, H)\n",
        "        self.linear3 = torch.nn.Linear(H, H)\n",
        "        self.linear4 = torch.nn.Linear(H, H)\n",
        "        self.linear5 = torch.nn.Linear(H, D_out)\n",
        "\n",
        "        # Define proportion or neurons to dropout\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "\n",
        "        x = F.relu(self.linear1(input))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.linear3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.linear4(x))\n",
        "        x = self.dropout(x)\n",
        "        y_pred = F.relu(self.linear5(x))\n",
        "\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "class RewardNet(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(RewardNet, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.linear2 = torch.nn.Linear(H, H)\n",
        "        self.linear3 = torch.nn.Linear(H, H)\n",
        "        self.linear4 = torch.nn.Linear(H, H)\n",
        "        self.linear5 = torch.nn.Linear(H, D_out)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "       \n",
        "        x = F.relu(self.linear1(input))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        x = F.relu(self.linear4(x))\n",
        "        y_pred = torch.tanh(self.linear5(x))\n",
        "\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "    # must override two of the subclass functions:\n",
        "    def __len__(self):\n",
        "        # return len(self.samples)\n",
        "        return self.samples.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # return self.samples[idx]\n",
        "        return self.samples[idx,:]\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "\n",
        "        n = m.in_features\n",
        "        y = 1.0/np.sqrt(n)\n",
        "        m.weight.data.uniform_(-y, y)\n",
        "        m.bias.data.fill_(0.001)\n",
        "        # m.bias.data.uniform_(0, y)\n",
        "\n",
        "        # torch.nn.init.xavier_uniform_(m.weight)\n",
        "        # m.bias.data.fill_(0.01)\n",
        "\n",
        "        # How to call this function:\n",
        "        # net.apply(init_weights)\n"
      ],
      "metadata": {
        "id": "HfMx4Qom50xo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gauge.py"
      ],
      "metadata": {
        "id": "bRQigffmnAw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gauge_function(V, G, H):\n",
        "    \"\"\"\n",
        "    The gauge function of the vector z w.r.t. the set P\n",
        "    is given by the following code.\n",
        "\n",
        "    V can be batched, for example, r-dimensional and batch size of K, then shape(V) = (r, K)\n",
        "    Note that the second dimension is the batch.\n",
        "\n",
        "    P is defined by {v: G@v <= Hj} = {v: g_i^T@v <= Hj_i, i = 1, ..., q}\n",
        "    shape(G) = (q, r), shape(Hj) = (q, 1), shape(H) = (q, K)\n",
        "    P must contain the origin in its interior.\n",
        "    \n",
        "    \"\"\"\n",
        "    # return torch.max(G@V/H,dim = 0).values # shape(output) = (1, K)\n",
        "\n",
        "    # torch.div() for element-wide division\n",
        "    return torch.max(torch.div(G@V, H),dim = 0).values # shape(output) = (1, K)\n",
        "\n",
        "def gauge_map(Z, G, H):\n",
        "    \"\"\"\n",
        "    For any Z \\belongsto B_infinity, the gauge map from B_infinity to the set P\n",
        "    defined by {v: G@v <= h} is given by the following code.\n",
        "    \n",
        "    Z can be batched, for example, r-dimensional and batch size of K, then shape(V) = (r, K)\n",
        "    Note that the second dimension is the batch.\n",
        "\n",
        "    P is defined by {v: G@v <= Hj} = {v: g_i^T@v <= Hj_i, i = 1, ..., q}\n",
        "    shape(G) = (q, r), shape(Hj) = (q, 1), shape(H) = (q, K)\n",
        "    P must contain the origin in its interior.\n",
        "    \"\"\"\n",
        "\n",
        "    gamma_dest = gauge_function(Z, G, H)# shape = (1, K)\n",
        "    # print('gamma_dest:', gamma_dest)\n",
        "    gamma_start = torch.linalg.norm(Z, ord = np.inf, dim=0) # shape(1, K)\n",
        "\n",
        "    scaling_mat = torch.diag(gamma_start/gamma_dest) # shape = (K, K)\n",
        "\n",
        "    return Z@scaling_mat # shape = (r, K), this is the new point in P"
      ],
      "metadata": {
        "id": "EQ1qXNOgnCXV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## system.py"
      ],
      "metadata": {
        "id": "YCN03qDPnEpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_unique_lines(connections):\n",
        "    all_lines = {}\n",
        "    count = 0\n",
        "    for line in connections:\n",
        "        all_lines[count] = line\n",
        "        count+=1\n",
        "\n",
        "    # This code snippet only finds out the repeated lines with exactly the same order of nodes,\n",
        "    # but not deal with that [i,j] and [j,i] are also repeated lines\n",
        "    # By checking connections, there is no repeated lines like [i,j] and [j,i]\n",
        "    unique_lines = {}\n",
        "    for k, val in all_lines.items():\n",
        "        if val not in unique_lines.values():\n",
        "            unique_lines[k]=val\n",
        "    print('unique_lines length:', len(unique_lines))\n",
        "\n",
        "    repeated_lines = [[42, 49],[49, 54],[56, 59],[49, 66],[77, 80],[89, 90],[89, 92]]\n",
        "    # For example, [42, 49] appears twice\n",
        "    set1 = {}\n",
        "    set2 = {}\n",
        "    for k, val in all_lines.items():\n",
        "        if val in repeated_lines and k in unique_lines:\n",
        "            set1[val[0], val[1]] = k # Record the repeated lines when they first appear\n",
        "        if val in repeated_lines and k not in unique_lines:\n",
        "            set2[val[0], val[1]] = k # Record the repeated lines when they appear more than once\n",
        "\n",
        "    # print('set1:', len(set1))\n",
        "    # print('set2:', len(set2))\n",
        "\n",
        "    return unique_lines, set1, set2\n",
        "\n",
        "\n",
        "def get_Y(N, B):\n",
        "    # N = num_buses\n",
        "    Y = np.zeros((N, N))\n",
        "    for i in range(N):\n",
        "        for j in range(N):\n",
        "            if i==j : \n",
        "                Y[i,j] = sum(B[i,:])\n",
        "            else: \n",
        "                Y[i,j] = -B[i,j]\n",
        "\n",
        "    return Y[:,1:]\n",
        "\n",
        "\n",
        "def get_A(N, L, B, connections):\n",
        "    # N = num_buses\n",
        "    # L = num_lines \n",
        "    A = np.zeros((L, N))\n",
        "\n",
        "    for i, line in enumerate(connections):\n",
        "        row = line[0]-1\n",
        "        col = line[1]-1\n",
        "        A[i, row] = B[row,col]\n",
        "        A[i, col] = -B[row,col]\n",
        "\n",
        "    return A[:,1:]\n",
        "\n",
        "\n",
        "# Import 118bus\n",
        "def import_118bus(params_path):\n",
        "    bus_data_fname = '118bus_BusData.csv'\n",
        "    gen_data_fname = '118bus_GenData.csv'\n",
        "\n",
        "    branch_data_fname = '118bus_BranchData.csv'\n",
        "    cost_data_fname = '118bus_CostData.csv'\n",
        "\n",
        "    bus_data_df = pd.read_csv(params_path+bus_data_fname,header=None)\n",
        "    gen_data_df = pd.read_csv(params_path+gen_data_fname,header=None)\n",
        "    branch_data_df = pd.read_csv(params_path+branch_data_fname,header=None)\n",
        "    cost_data_df = pd.read_csv(params_path+cost_data_fname,header=None)\n",
        "\n",
        "\n",
        "    num_buses = bus_data_df.shape[0]\n",
        "    num_lines = branch_data_df.shape[0]\n",
        "    num_gens = gen_data_df.shape[0]\n",
        "\n",
        "\n",
        "    bus_data = bus_data_df.to_numpy()\n",
        "    gen_data = gen_data_df.to_numpy()\n",
        "    branch_data = branch_data_df.to_numpy()\n",
        "    cost_data = cost_data_df.to_numpy()\n",
        "\n",
        "    x = branch_data[:,3]\n",
        "    print('max of x:', max(x))\n",
        "    print('min of x:', min(x))\n",
        "\n",
        "\n",
        "    b = 1/x\n",
        "    Z0 = 10\n",
        "    b = b/Z0\n",
        "    print('max of b:', max(b))\n",
        "    print('min of b:', min(b))\n",
        "\n",
        "    connections = []\n",
        "    branches = branch_data[:,:2]\n",
        "    for i in range(branches.shape[0]):\n",
        "            connections.append([int(branches[i,0]),int(branches[i,1])])\n",
        "    print('len of connections:', len(connections))\n",
        "\n",
        "    unique_lines, set1, set2 = identify_unique_lines(connections)\n",
        "\n",
        "    B = np.zeros((num_buses, num_buses))\n",
        "    for k, line in unique_lines.items():\n",
        "        row = line[0]-1\n",
        "        col = line[1]-1\n",
        "        B[row, col] = b[k]\n",
        "        B[col, row] = b[k]\n",
        "\n",
        "    PD = bus_data[:,2]/20\n",
        "    print('Total PD:', sum(PD))\n",
        "\n",
        "    return num_buses, num_lines, B, connections, PD"
      ],
      "metadata": {
        "id": "BikI-OcI51au"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataloader.py"
      ],
      "metadata": {
        "id": "_XaV-g1PnHWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_inputs(Ntr, Ntst, Ntr2, scaling, nominal_PD_data, data_path):\n",
        "\n",
        "\n",
        "    forecasts = nominal_PD_data.reshape(1,-1)\n",
        "\n",
        "    lb = np.clip((1-scaling)*forecasts, a_min=0., a_max=None)\n",
        "    ub = (1+scaling)*forecasts\n",
        "\n",
        "    suffix = str(int(scaling*100))+'percent.npy'\n",
        "    print('suffix:', suffix)\n",
        "\n",
        "\n",
        "    train_dataset = np.random.uniform(lb, ub, (Ntr, N))\n",
        "    test_dataset = np.random.uniform(lb, ub, (Ntst, N))\n",
        "\n",
        "    # Generate dataset for pretrain\n",
        "    random_rows = np.random.choice(Ntr, Ntr2)\n",
        "    pretrain_dataset = train_dataset[random_rows,:]\n",
        "    print('pretrain_dataset:', pretrain_dataset.shape)\n",
        "\n",
        "    np.save(data_path+'train_set.npy', train_dataset)\n",
        "    np.save(data_path+'test_set.npy', test_dataset)\n",
        "    np.save(data_path+'pretrain_set.npy', pretrain_dataset)\n",
        "\n",
        "    return train_dataset, test_dataset, pretrain_dataset"
      ],
      "metadata": {
        "id": "vmth23da519X"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_dcopf(forecasts, x_vals):\n",
        "    '''\n",
        "    Given a single instance and the initial dispatch x, and evaluate the quality\n",
        "    of x.\n",
        "    This function can be applied to the decision obtained from any policy.\n",
        "    '''\n",
        "\n",
        "    def sinlge_instance(forecast, x):\n",
        "\n",
        "        theta = cp.Variable((N-1,1))\n",
        "        y = cp.Variable((N, 1))\n",
        "\n",
        "        constraints_list = []\n",
        "\n",
        "        if Pe == 'uniform':\n",
        "            lb = np.clip((1-ratio)*forecast, a_min=0., a_max=None)\n",
        "            ub = (1+ratio)*forecast\n",
        "            rt_scenario = np.random.uniform(lb, ub, (forecast.shape[0], forecast.shape[1]))\n",
        "\n",
        "        else:\n",
        "            rt_scenario = forecast + sigma*np.random.randn(N, 1) \n",
        "        \n",
        "        net_d_omega = rt_scenario - x\n",
        "\n",
        "        constraints_list.append( y == Yrr@theta + net_d_omega )\n",
        "            \n",
        "        constraints_list.append( Arr@theta <= Fmax*np.ones((num_lines, 1)))\n",
        "        constraints_list.append( Arr@theta >= -Fmax*np.ones((num_lines, 1)))\n",
        "\n",
        "        constraints_list.append( theta<=np.pi)\n",
        "        constraints_list.append( theta>=-np.pi)\n",
        "\n",
        "        Q_pred = cp.pos(y.T)@linear_cost_Coeff\n",
        "\n",
        "        prob = cp.Problem(cp.Minimize( Q_pred ), constraints_list)\n",
        "\n",
        "        prob.solve(verbose = False, solver = cp.ECOS) ## Works!   \n",
        "\n",
        "        return prob.value, y.value, theta.value, net_d_omega\n",
        "\n",
        "    num_points = forecasts.shape[0]\n",
        "\n",
        "    Q_vals = []\n",
        "    y_vals = []\n",
        "    theta_vals = []\n",
        "    net_d_vals = []\n",
        "    for i in range(num_points):\n",
        "        forecast = forecasts[i,:].reshape(-1,1)\n",
        "        x = x_vals[i,:].reshape(-1,1)\n",
        "\n",
        "        Q_omega, y_omega, theta_omega, net_d_omega = sinlge_instance(forecast, x)\n",
        "\n",
        "        Q_vals.append(Q_omega)\n",
        "        y_vals.append(y_omega.flatten())\n",
        "        theta_vals.append(theta_omega.flatten())\n",
        "        net_d_vals.append(net_d_omega.flatten())\n",
        "\n",
        "    Q_vals = np.array(Q_vals).reshape(-1,1)\n",
        "    y_vals = np.array(y_vals)\n",
        "    theta_vals = np.array(theta_vals)\n",
        "    net_d_vals = np.array(net_d_vals)\n",
        "\n",
        "    return Q_vals, y_vals, theta_vals, net_d_vals\n"
      ],
      "metadata": {
        "id": "eMTE7hboL-6k"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## benchmarks.py"
      ],
      "metadata": {
        "id": "FjLzC7u1nQS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def saa_solver(forecast, num_sce):\n",
        "    '''\n",
        "    Given a single instance of forecast, solve the SAA version\n",
        "    of the true problem.\n",
        "    This is called cp policy in our work.\n",
        "    '''\n",
        "\n",
        "    M = num_sce\n",
        "\n",
        "    x = cp.Variable((N, 1))\n",
        "    theta = cp.Variable((N-1,M))\n",
        "    y = cp.Variable((N, M))\n",
        "\n",
        "    constraints_list = []\n",
        "\n",
        "    for m in range(M):\n",
        "\n",
        "        if Pe == 'uniform':\n",
        "            lb = np.clip((1-ratio)*forecast, a_min=0., a_max=None)\n",
        "            ub = (1+ratio)*forecast\n",
        "            rt_scenario = np.random.uniform(lb, ub, (forecast.shape[0], forecast.shape[1]))\n",
        "\n",
        "        else:\n",
        "\n",
        "            rt_scenario = forecast + sigma*np.random.randn(N, 1) \n",
        "        \n",
        "        d_tilde = rt_scenario - x\n",
        "\n",
        "        constraints_list.append( y[:,m:m+1] == Yrr@theta[:,m:m+1] + d_tilde )\n",
        "        \n",
        "    constraints_list.append( Arr@theta <= Fmax*np.ones((num_lines, M)))\n",
        "    constraints_list.append( Arr@theta >= -Fmax*np.ones((num_lines, M)))\n",
        "\n",
        "    constraints_list.append( theta<=np.pi)\n",
        "    constraints_list.append( theta>=-np.pi)\n",
        "    constraints_list.append( x>=0)\n",
        "\n",
        "    Q_vals = []\n",
        "    for m in range(M):\n",
        "\n",
        "        Q_vals.append(cp.pos(y[:,m:m+1].T)@linear_cost_Coeff)\n",
        "\n",
        "    Q_pred = cp.sum(Q_vals)/M\n",
        "    prob = cp.Problem(cp.Minimize( da_cost_Coeff.T@x + Q_pred ), constraints_list)\n",
        "   \n",
        "    prob.solve(verbose = False, solver = cp.ECOS) ## Works!\n",
        "\n",
        "    return x.value.T, prob.value\n",
        "\n",
        "\n",
        "def ap_solver(forecast, num_sce):\n",
        "    '''\n",
        "    Given a single instance of forecast, solve the SAA version\n",
        "    of the true problem, but with theta approximated by affine policy.\n",
        "    This is called ap policy in our work.\n",
        "    '''\n",
        "\n",
        "    M = num_sce\n",
        "\n",
        "    c = da_cost_Coeff\n",
        "    q = linear_cost_Coeff\n",
        "    B = -Yrr\n",
        "    F = Arr\n",
        "    rho_lb = 10.\n",
        "    rho_ub = 10.\n",
        "\n",
        "    \n",
        "    Pi = cp.Variable((N-1,N))\n",
        "    beta = cp.Variable((N-1,1))\n",
        "\n",
        "    x = cp.Variable((N, 1))\n",
        "\n",
        "\n",
        "    constraints_list = []\n",
        "\n",
        "    Q_vals = []\n",
        "\n",
        "    for m in range(M):\n",
        "        # for each scenario of noise\n",
        "        if Pe == 'uniform':\n",
        "            lb = np.clip((1-ratio)*forecast, a_min=0., a_max=None)\n",
        "            ub = (1+ratio)*forecast\n",
        "            rt_scenario = np.random.uniform(lb, ub, (forecast.shape[0], forecast.shape[1]))\n",
        "\n",
        "        else:\n",
        "\n",
        "            rt_scenario = forecast + sigma*np.random.randn(N, 1) \n",
        "\n",
        "        noise_sce =  rt_scenario - forecast\n",
        "        net_d = rt_scenario - x\n",
        "\n",
        "        theta_pred = Pi@noise_sce + beta\n",
        "        y_pred = net_d - B@theta_pred\n",
        "\n",
        "        p_f_lb = rho_lb*cp.norm( cp.pos(-Fmax*np.ones((num_lines, 1)) - F@theta_pred) )\n",
        "        p_f_ub = rho_ub*cp.norm( cp.pos(F@theta_pred -Fmax*np.ones((num_lines, 1))) )\n",
        "\n",
        "        p_v_lb  = rho_lb*cp.norm( cp.pos(-np.pi*np.ones((N-1, 1)) - theta_pred) )\n",
        "        p_v_ub  = rho_ub*cp.norm( cp.pos(theta_pred - np.pi*np.ones((N-1, 1))) )\n",
        "\n",
        "        Q_val = q.T@cp.pos(y_pred) + p_f_lb + p_f_ub + p_v_lb + p_v_ub\n",
        "\n",
        "        Q_vals.append(Q_val)\n",
        "\n",
        "    constraints_list.append( x>=0)\n",
        "\n",
        "\n",
        "    Q_pred = cp.sum(Q_vals)/M\n",
        "\n",
        "    prob = cp.Problem(cp.Minimize( c.T@x + Q_pred ), constraints_list)\n",
        "\n",
        "    prob.solve(verbose = False, solver = cp.ECOS) ## Works!  \n",
        "\n",
        "    assert prob.status not in [\"infeasible\", \"unbounded\"], \"Problem is not feasible or unbounded.\"\n",
        "\n",
        "\n",
        "    return x.value.T, prob.value\n",
        "\n",
        "\n",
        "def solver_outer_loop(forecasts, num_sce, solver_name):\n",
        "    '''\n",
        "    Given a batch of instances and solve rld problem for each instance.\n",
        "    We only care about x for now.\n",
        "    Can return computation time for each instance at the same time (in minutes)\n",
        "    '''\n",
        "\n",
        "    num_points = forecasts.shape[0]\n",
        "\n",
        "    x_vals = []\n",
        "    total_cost_vals = []\n",
        "    times = []\n",
        "\n",
        "\n",
        "    for it in range(num_points):\n",
        "\n",
        "        forecast = forecasts[it,:].reshape(-1,1)\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        if solver_name == 'saa':\n",
        "            x_value, prob_value = saa_solver(forecast, num_sce)\n",
        "\n",
        "        elif solver_name == 'ap':\n",
        "            x_value, prob_value = ap_solver(forecast, num_sce)\n",
        "\n",
        "        else:\n",
        "            print('No such solver exists!')\n",
        "\n",
        "        end_time = time.time()\n",
        "        times.append((end_time-start_time)/60)\n",
        "\n",
        "        x_vals.append(x_value.flatten())\n",
        "        total_cost_vals.append(prob_value)\n",
        "\n",
        "        # print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
        "        \n",
        "    x_vals = np.array(x_vals)\n",
        "    total_cost_vals = np.array(total_cost_vals).reshape(-1,1)\n",
        "    times = np.array(times)\n",
        "\n",
        "    return x_vals, total_cost_vals, times\n",
        "\n",
        "\n",
        "def evaluate_Q(forecast, x, num_sce):\n",
        "    '''\n",
        "    Given a single instance and the initial dispatch x, and evaluate the quality\n",
        "    of x.\n",
        "    This function can be applied to the decision obtained from any policy.\n",
        "    '''\n",
        "\n",
        "    M = num_sce\n",
        "\n",
        "    theta = cp.Variable((N-1,M))\n",
        "    y = cp.Variable((N, M))\n",
        "\n",
        "    constraints_list = []\n",
        "\n",
        "    for m in range(M):\n",
        "        if Pe == 'uniform':\n",
        "            lb = np.clip((1-ratio)*forecast, a_min=0., a_max=None)\n",
        "            ub = (1+ratio)*forecast\n",
        "            rt_scenario = np.random.uniform(lb, ub, (forecast.shape[0], forecast.shape[1]))\n",
        "\n",
        "        else:\n",
        "\n",
        "            rt_scenario = forecast + sigma*np.random.randn(N, 1) \n",
        "        \n",
        "        net_d = rt_scenario - x\n",
        "\n",
        "        constraints_list.append( y[:,m:m+1] == Yrr@theta[:,m:m+1] + net_d )\n",
        "        \n",
        "    constraints_list.append( Arr@theta <= Fmax*np.ones((num_lines, M)))\n",
        "    constraints_list.append( Arr@theta >= -Fmax*np.ones((num_lines, M)))\n",
        "\n",
        "    constraints_list.append( theta<=np.pi)\n",
        "    constraints_list.append( theta>=-np.pi)\n",
        "\n",
        "    Q_vals = []\n",
        "    for m in range(M):\n",
        "\n",
        "        Q_vals.append(cp.pos(y[:,m:m+1].T)@linear_cost_Coeff)\n",
        "\n",
        "    Q_pred = cp.sum(Q_vals)/M\n",
        "\n",
        "    prob = cp.Problem(cp.Minimize( Q_pred ), constraints_list)\n",
        "\n",
        "    prob.solve(verbose = False, solver = cp.ECOS) ## Works!   \n",
        "\n",
        "    return prob.value, np.sum(y.value, axis=1)/M\n",
        "\n",
        "\n",
        "def evaluate_outer_loop(forecasts, x_pred, num_sce):\n",
        "    '''\n",
        "    Given a batch of instances and the associated predictions of first stage decision,\n",
        "    and evaluate these initial dispatch decisions for each instance.\n",
        "\n",
        "    '''\n",
        "\n",
        "    num_points = forecasts.shape[0]\n",
        "\n",
        "    Q_pred = []\n",
        "    y_pred = []\n",
        "\n",
        "    for i in range(num_points):\n",
        "        forecast = forecasts[i,:].reshape(-1,1)\n",
        "        init_dispatch = x_pred[i,:].reshape(-1,1)\n",
        "\n",
        "        Q_value, avg_y_value = evaluate_Q(forecast, init_dispatch, num_sce)\n",
        "\n",
        "        Q_pred.append(Q_value)\n",
        "        y_pred.append(avg_y_value.flatten())\n",
        "        \n",
        "\n",
        "    Q_pred = np.array(Q_pred).reshape(-1,1)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    total_cost_pred = x_pred@da_cost_Coeff + Q_pred\n",
        "\n",
        "    return total_cost_pred, Q_pred, y_pred\n"
      ],
      "metadata": {
        "id": "jKB5S8_UnUyV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main.py"
      ],
      "metadata": {
        "id": "GA5iXdh_6f71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define params_path, saved_path, data_path, and model_path here\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "6FzjzTKm69Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_paths():\n",
        "    params_path = root_path+'118bus/params/'\n",
        "    data_path = root_path+'118bus/simulations/data/'\n",
        "    saved_path = root_path+'118bus/simulations/results/'\n",
        "    model_path = root_path+'118bus/rld_simulations/saved_models/'\n",
        "\n",
        "\n",
        "    isExist = os.path.exists(data_path)\n",
        "    if not isExist:\n",
        "        # Create a new directory if it does not exist \n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    \n",
        "    isExist = os.path.exists(saved_path)\n",
        "    if not isExist:\n",
        "        # Create a new directory if it does not exist \n",
        "        os.makedirs(saved_path)\n",
        "\n",
        "    return params_path, data_path, saved_path, model_path"
      ],
      "metadata": {
        "id": "1d257ZpX7C3j"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import 118bus system"
      ],
      "metadata": {
        "id": "KTuqyFru6lsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_saved_models(model_path):\n",
        "\n",
        "    action_net = ActionNet(N, 256, N)\n",
        "    action_net.load_state_dict(torch.load(model_path+'trained_action_net.pt'))\n",
        "\n",
        "    reward_net = RewardNet(N, 256, N-1)\n",
        "    reward_net.load_state_dict(torch.load(model_path+'trained_reward_net.pt'))\n",
        "\n",
        "    return action_net, reward_net\n",
        "\n",
        "\n",
        "def load_pretrain_models(model_path):\n",
        "\n",
        "    action_net = ActionNet(N, 256, N)\n",
        "    action_net.load_state_dict(torch.load(model_path+'trained_action_net_sl.pt'))\n",
        "\n",
        "    reward_net = RewardNet(N, 256, N-1)\n",
        "    reward_net.load_state_dict(torch.load(model_path+'trained_reward_net_sl.pt'))\n",
        "\n",
        "    return action_net, reward_net\n",
        "\n",
        "\n",
        "def load_cost_coeff(params_path):    \n",
        "    quad_cost_coeff = np.load(params_path+'quad_cost_coeff.npy')\n",
        "    linear_cost_coeff = np.load(params_path+'linear_cost_coeff.npy')\n",
        "    da_cost_coeff = np.load(params_path+'da_cost_coeff.npy')\n",
        "\n",
        "    return quad_cost_coeff, linear_cost_coeff, da_cost_coeff\n",
        "\n",
        "\n",
        "params_path, data_path, saved_path, model_path = get_paths()\n",
        "\n",
        "\n",
        "num_buses, num_lines, B, connections, PD_data = import_118bus(params_path)\n",
        "\n",
        "N = num_buses\n",
        "L = num_lines \n",
        "\n",
        "Yrr = get_Y(num_buses, B)\n",
        "Arr = get_A(num_buses, num_lines, B, connections)\n",
        "\n",
        "# Define feasibility set for gauge mapping\n",
        "G = np.block([\n",
        "              [Arr],\n",
        "              [-Arr],\n",
        "              [np.eye(N-1)],\n",
        "              [-np.eye(N-1)]\n",
        "])\n",
        "print('G shape:', G.shape)\n",
        "\n",
        "\n",
        "Fmax = 1.\n",
        "\n",
        "# If forecast error follows gaussian distribution with zero mean and std sigma\n",
        "Pe = 'gaussian'\n",
        "sigma = 1.\n",
        "\n",
        "# If forecast error follows interval bounded distribution-uniform then ratio is 5%\n",
        "# Pe = 'uniform'\n",
        "ratio = 0.05\n",
        "\n",
        "# Load cost coefficients\n",
        "quad_cost_coeff, linear_cost_coeff, da_cost_coeff = load_cost_coeff(params_path) \n",
        "\n",
        "quad_cost_Coeff = np.diag(quad_cost_coeff)\n",
        "linear_cost_Coeff = linear_cost_coeff.reshape(-1, 1)\n",
        "da_cost_Coeff = da_cost_coeff.reshape(-1,1)\n",
        "\n",
        "print('quad_cost_Coeff shape:', quad_cost_Coeff.shape)\n",
        "print('linear_cost_Coeff shape:', linear_cost_Coeff.shape)\n",
        "print('da_cost_Coeff shape:', da_cost_Coeff.shape)"
      ],
      "metadata": {
        "id": "En0JQv5h6pcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9dcc7ee-13d3-495a-b4b5-ce9967ed717e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max of x: 0.411\n",
            "min of x: 0.004\n",
            "max of b: 25.0\n",
            "min of b: 0.24330900243309003\n",
            "len of connections: 186\n",
            "unique_lines length: 179\n",
            "Total PD: 212.10000000000005\n",
            "quad_cost_Coeff shape: (118, 118)\n",
            "linear_cost_Coeff shape: (118, 1)\n",
            "da_cost_Coeff shape: (118, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct datasets"
      ],
      "metadata": {
        "id": "-axBx5c9F2RW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct train_set, test_set, pretrain_set if needed"
      ],
      "metadata": {
        "id": "tXxVw37GGob7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nominal_PD_data = PD_data.reshape(1,-1)\n",
        "\n",
        "# Load dataset\n",
        "scaling = 0.1\n",
        "Ntr = 50000\n",
        "Ntr2 = 100\n",
        "Ntst = 100\n",
        "\n",
        "# _, test_dataset, pretrain_dataset =generate_inputs(Ntr, Ntst, Ntr2,\n",
        "#                                                     scaling, nominal_PD_data, \n",
        "#                                                     data_path)\n",
        "\n",
        "train_dataset = np.load(data_path+'train_set.npy')\n",
        "test_dataset = np.load(data_path+'test_set.npy')\n",
        "pretrain_dataset = np.load(data_path+'pretrain_set.npy')\n",
        "\n",
        "max_forecast_val = np.max(train_dataset)\n",
        "# Note that the original train_dataset and test_dataset are not normalized\n",
        "TrainSet = TrainDataset(train_dataset/max_forecast_val)\n",
        "TestSet = TrainDataset(test_dataset/max_forecast_val)\n"
      ],
      "metadata": {
        "id": "g1PKjAMWLCHC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct dataset for pretrain\n",
        "\n",
        "\n",
        "*   X_train, X_test for action net training\n",
        "*   X_train2, X_test2 for reward net training\n",
        "\n",
        "\n",
        "### Format:\n",
        "\n",
        "*   X_train, X_test: [forecasts, x_true]\n",
        "*   X_train2, X_test2:[forecasts-x_true, theta_true, y_true, Q_true]\n"
      ],
      "metadata": {
        "id": "-NRrwiTOGtxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_ap = 10\n",
        "N_cp = 500\n",
        "N_eval = 500"
      ],
      "metadata": {
        "id": "HGZKbF-sHt7s"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_true, _, _ = solver_outer_loop(pretrain_dataset, num_sce=N_cp, solver_name='saa')\n",
        "X_tr = np.concatenate([pretrain_dataset, x_true], axis=-1)/np.max(pretrain_dataset)\n",
        "X_train = X_tr[:90,:]\n",
        "X_test = X_tr[90:,:]\n",
        "np.save(data_path+'X_train.npy', X_train)\n",
        "np.save(data_path+'X_test.npy', X_test)"
      ],
      "metadata": {
        "id": "aR5B5Y7cHMEM"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_omega, y_omega, theta_omega, net_d_omega = stochastic_dcopf(pretrain_dataset, x_true)\n",
        "X_tr2 = np.concatenate([net_d_omega, theta_omega, y_omega, Q_omega], axis=-1)\n",
        "X_train2 = X_tr2[:90,:]\n",
        "X_test2 = X_tr2[90:,:]\n",
        "np.save(data_path+'X_train2.npy', X_train2)\n",
        "np.save(data_path+'X_test2.npy', X_test2)"
      ],
      "metadata": {
        "id": "Di8iKgTOOWQc"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# benchmarks.py"
      ],
      "metadata": {
        "id": "12dGMMoSHOtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmark1: use cvxpy to solve saa version of the true problem"
      ],
      "metadata": {
        "id": "FgyyVHVuHbBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_cp, _, cp_times = solver_outer_loop(test_dataset, num_sce=N_cp, solver_name='saa')\n",
        "np.save(data_path+'x_cp.npy', x_cp)\n",
        "np.save(data_path+'cp_times.npy', cp_times)"
      ],
      "metadata": {
        "id": "ShyD_PApHeEb"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate using evaluate_outer_loop(forecasts, x_pred)\n",
        "total_cost_cp, Q_cp, y_cp = evaluate_outer_loop(test_dataset, x_cp, num_sce=N_eval)\n",
        "\n",
        "np.save(data_path+'total_cost_cp.npy', total_cost_cp)\n",
        "np.save(data_path+'Q_cp.npy', Q_cp)\n",
        "np.save(data_path+'y_cp.npy', y_cp)"
      ],
      "metadata": {
        "id": "LKJ4jZyMHrJ3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmark2: solve the saa version with affine policy plugged in"
      ],
      "metadata": {
        "id": "jiHJCh5GHeOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_ap, _, ap_times = solver_outer_loop(test_dataset, num_sce=N_ap, solver_name='ap')\n",
        "np.save(data_path+'x_ap.npy', x_ap)\n",
        "np.save(data_path+'ap_times.npy', ap_times)\n",
        "x_ap = np.load(data_path+'x_ap.npy')"
      ],
      "metadata": {
        "id": "k2dViwBnHhmL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate using evaluate_outer_loop(forecasts, x_pred)\n",
        "total_cost_ap, Q_ap, y_ap = evaluate_outer_loop(test_dataset, x_ap, num_sce=N_eval)\n",
        "\n",
        "np.save(data_path+'total_cost_ap.npy', total_cost_ap)\n",
        "np.save(data_path+'Q_ap.npy', Q_ap)\n",
        "np.save(data_path+'y_ap.npy', y_ap)"
      ],
      "metadata": {
        "id": "GYANlL3wIML-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# plotting"
      ],
      "metadata": {
        "id": "FMyAVMDEReae"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qu07MjxKRe8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}