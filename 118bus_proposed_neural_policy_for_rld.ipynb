{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPznE2HAD6ZwbPaPjBygdWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhhhling/risk_limiting_dispatchSept/blob/main/118bus_proposed_neural_policy_for_rld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q3UWPo6mISo",
        "outputId": "2a1ed867-ab3a-4880-9aaa-52487bd75e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in case gpu is used\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QCAsDEcmR1O",
        "outputId": "6b6a9755-8559-4542-d503-ff2d070d2a6e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "import cvxpy as cp\n",
        "import random\n",
        "from numpy import savetxt\n",
        "import argparse\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "root_path = './gdrive/MyDrive/Inbox/rld_neural_policy/'"
      ],
      "metadata": {
        "id": "mvxphpwn1Th-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# helper functions, can be folded"
      ],
      "metadata": {
        "id": "7AfB0oo7mnIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils.py"
      ],
      "metadata": {
        "id": "JRt8IRhLmx8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dir(PATH):\n",
        "    isExist = os.path.exists(PATH)\n",
        "\n",
        "    if not isExist:\n",
        "        # Create a new directory if it does not exist \n",
        "        os.makedirs(PATH)\n",
        "        print(\"The new directory is created!\")\n",
        "\n",
        "\n",
        "# Evaluate using vector distance\n",
        "def measure_relative_distance(v1, v2):\n",
        "    '''\n",
        "        Note that v1 is the benchmark.\n",
        "        Norm is calculated along dimension/axis 1\n",
        "        and average is calculated along dimension/axis 0\n",
        "        Also return the distance vetor.\n",
        "    '''\n",
        "    if len(v1.shape)==1 and len(v2.shape)==1:\n",
        "        distance = np.abs(v1-v2)/np.abs(v1)\n",
        "\n",
        "    if len(v1.shape)==2 and len(v2.shape)==2:\n",
        "        distance = np.linalg.norm(v1-v2, axis=1)/np.linalg.norm(v1, axis=1)\n",
        "\n",
        "    return distance, np.mean(distance)"
      ],
      "metadata": {
        "id": "R4SwyMaKmmou"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plot.py"
      ],
      "metadata": {
        "id": "HEtK8hChA-YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_loss(train_hist, epoch, fig_name, yplot):\n",
        "        print('len:', len(train_hist['train_losses']))\n",
        "\n",
        "        fig = plt.figure(figsize=(10,10))\n",
        "        plt.ticklabel_format(style='plain', scilimits=(-3, 3), useOffset=False)\n",
        "        ax = fig.add_subplot(1, 1, 1)\n",
        "        if yplot == 'log10':\n",
        "            ax.plot(range(len(train_hist['train_losses'])), np.log10(train_hist['train_losses']), label='train loss')\n",
        "            ax.set_ylabel('log10(train loss)',fontsize=20)\n",
        "        elif yplot == 'plain':\n",
        "            ax.plot(range(len(train_hist['train_losses'])), train_hist['train_losses'], label='train loss')\n",
        "            ax.set_ylabel('train loss',fontsize=20)\n",
        "        ax.set_xlabel('training epochs',fontsize=20)\n",
        "        plt.xticks(fontsize=20)\n",
        "        plt.yticks(fontsize=20)\n",
        "        plt.savefig(saved_path+str(epoch)+fig_name)"
      ],
      "metadata": {
        "id": "4bM-tWLfA_ad"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## models.py"
      ],
      "metadata": {
        "id": "PXvt9h5Rm_cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionNet(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(ActionNet, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.linear2 = torch.nn.Linear(H, H)\n",
        "        self.linear3 = torch.nn.Linear(H, H)\n",
        "        self.linear4 = torch.nn.Linear(H, H)\n",
        "        self.linear5 = torch.nn.Linear(H, D_out)\n",
        "\n",
        "        # Define proportion or neurons to dropout\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "\n",
        "        x = F.relu(self.linear1(input))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.linear3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.linear4(x))\n",
        "        x = self.dropout(x)\n",
        "        y_pred = F.relu(self.linear5(x))\n",
        "\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "class RewardNet(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(RewardNet, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.linear2 = torch.nn.Linear(H, H)\n",
        "        self.linear3 = torch.nn.Linear(H, H)\n",
        "        self.linear4 = torch.nn.Linear(H, H)\n",
        "        self.linear5 = torch.nn.Linear(H, D_out)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "       \n",
        "        x = F.relu(self.linear1(input))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        x = F.relu(self.linear4(x))\n",
        "        y_pred = torch.tanh(self.linear5(x))\n",
        "\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "    # must override two of the subclass functions:\n",
        "    def __len__(self):\n",
        "        # return len(self.samples)\n",
        "        return self.samples.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # return self.samples[idx]\n",
        "        return self.samples[idx,:]\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "\n",
        "        n = m.in_features\n",
        "        y = 1.0/np.sqrt(n)\n",
        "        m.weight.data.uniform_(-y, y)\n",
        "        m.bias.data.fill_(0.001)\n",
        "        # m.bias.data.uniform_(0, y)\n",
        "\n",
        "        # torch.nn.init.xavier_uniform_(m.weight)\n",
        "        # m.bias.data.fill_(0.01)\n",
        "\n",
        "        # How to call this function:\n",
        "        # net.apply(init_weights)\n"
      ],
      "metadata": {
        "id": "HfMx4Qom50xo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gauge.py"
      ],
      "metadata": {
        "id": "bRQigffmnAw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gauge_function(V, G, H):\n",
        "    \"\"\"\n",
        "    The gauge function of the vector z w.r.t. the set P\n",
        "    is given by the following code.\n",
        "\n",
        "    V can be batched, for example, r-dimensional and batch size of K, then shape(V) = (r, K)\n",
        "    Note that the second dimension is the batch.\n",
        "\n",
        "    P is defined by {v: G@v <= Hj} = {v: g_i^T@v <= Hj_i, i = 1, ..., q}\n",
        "    shape(G) = (q, r), shape(Hj) = (q, 1), shape(H) = (q, K)\n",
        "    P must contain the origin in its interior.\n",
        "    \n",
        "    \"\"\"\n",
        "    # return torch.max(G@V/H,dim = 0).values # shape(output) = (1, K)\n",
        "\n",
        "    # torch.div() for element-wide division\n",
        "    return torch.max(torch.div(G@V, H),dim = 0).values # shape(output) = (1, K)\n",
        "\n",
        "def gauge_map(Z, G, H):\n",
        "    \"\"\"\n",
        "    For any Z \\belongsto B_infinity, the gauge map from B_infinity to the set P\n",
        "    defined by {v: G@v <= h} is given by the following code.\n",
        "    \n",
        "    Z can be batched, for example, r-dimensional and batch size of K, then shape(V) = (r, K)\n",
        "    Note that the second dimension is the batch.\n",
        "\n",
        "    P is defined by {v: G@v <= Hj} = {v: g_i^T@v <= Hj_i, i = 1, ..., q}\n",
        "    shape(G) = (q, r), shape(Hj) = (q, 1), shape(H) = (q, K)\n",
        "    P must contain the origin in its interior.\n",
        "    \"\"\"\n",
        "\n",
        "    gamma_dest = gauge_function(Z, G, H)# shape = (1, K)\n",
        "    # print('gamma_dest:', gamma_dest)\n",
        "    gamma_start = torch.linalg.norm(Z, ord = np.inf, dim=0) # shape(1, K)\n",
        "\n",
        "    scaling_mat = torch.diag(gamma_start/gamma_dest) # shape = (K, K)\n",
        "\n",
        "    return Z@scaling_mat # shape = (r, K), this is the new point in P"
      ],
      "metadata": {
        "id": "EQ1qXNOgnCXV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## system.py"
      ],
      "metadata": {
        "id": "YCN03qDPnEpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_unique_lines(connections):\n",
        "    all_lines = {}\n",
        "    count = 0\n",
        "    for line in connections:\n",
        "        all_lines[count] = line\n",
        "        count+=1\n",
        "\n",
        "    # This code snippet only finds out the repeated lines with exactly the same order of nodes,\n",
        "    # but not deal with that [i,j] and [j,i] are also repeated lines\n",
        "    # By checking connections, there is no repeated lines like [i,j] and [j,i]\n",
        "    unique_lines = {}\n",
        "    for k, val in all_lines.items():\n",
        "        if val not in unique_lines.values():\n",
        "            unique_lines[k]=val\n",
        "    print('unique_lines length:', len(unique_lines))\n",
        "\n",
        "    repeated_lines = [[42, 49],[49, 54],[56, 59],[49, 66],[77, 80],[89, 90],[89, 92]]\n",
        "    # For example, [42, 49] appears twice\n",
        "    set1 = {}\n",
        "    set2 = {}\n",
        "    for k, val in all_lines.items():\n",
        "        if val in repeated_lines and k in unique_lines:\n",
        "            set1[val[0], val[1]] = k # Record the repeated lines when they first appear\n",
        "        if val in repeated_lines and k not in unique_lines:\n",
        "            set2[val[0], val[1]] = k # Record the repeated lines when they appear more than once\n",
        "\n",
        "    # print('set1:', len(set1))\n",
        "    # print('set2:', len(set2))\n",
        "\n",
        "    return unique_lines, set1, set2\n",
        "\n",
        "\n",
        "def get_Y(N, B):\n",
        "    # N = num_buses\n",
        "    Y = np.zeros((N, N))\n",
        "    for i in range(N):\n",
        "        for j in range(N):\n",
        "            if i==j : \n",
        "                Y[i,j] = sum(B[i,:])\n",
        "            else: \n",
        "                Y[i,j] = -B[i,j]\n",
        "\n",
        "    return Y[:,1:]\n",
        "\n",
        "\n",
        "def get_A(N, L, B, connections):\n",
        "    # N = num_buses\n",
        "    # L = num_lines \n",
        "    A = np.zeros((L, N))\n",
        "\n",
        "    for i, line in enumerate(connections):\n",
        "        row = line[0]-1\n",
        "        col = line[1]-1\n",
        "        A[i, row] = B[row,col]\n",
        "        A[i, col] = -B[row,col]\n",
        "\n",
        "    return A[:,1:]\n",
        "\n",
        "\n",
        "# Import 118bus\n",
        "def import_118bus(params_path):\n",
        "    bus_data_fname = '118bus_BusData.csv'\n",
        "    gen_data_fname = '118bus_GenData.csv'\n",
        "\n",
        "    branch_data_fname = '118bus_BranchData.csv'\n",
        "    cost_data_fname = '118bus_CostData.csv'\n",
        "\n",
        "    bus_data_df = pd.read_csv(params_path+bus_data_fname,header=None)\n",
        "    gen_data_df = pd.read_csv(params_path+gen_data_fname,header=None)\n",
        "    branch_data_df = pd.read_csv(params_path+branch_data_fname,header=None)\n",
        "    cost_data_df = pd.read_csv(params_path+cost_data_fname,header=None)\n",
        "\n",
        "\n",
        "    num_buses = bus_data_df.shape[0]\n",
        "    num_lines = branch_data_df.shape[0]\n",
        "    num_gens = gen_data_df.shape[0]\n",
        "\n",
        "\n",
        "    bus_data = bus_data_df.to_numpy()\n",
        "    gen_data = gen_data_df.to_numpy()\n",
        "    branch_data = branch_data_df.to_numpy()\n",
        "    cost_data = cost_data_df.to_numpy()\n",
        "\n",
        "    x = branch_data[:,3]\n",
        "    print('max of x:', max(x))\n",
        "    print('min of x:', min(x))\n",
        "\n",
        "\n",
        "    b = 1/x\n",
        "    Z0 = 10\n",
        "    b = b/Z0\n",
        "    print('max of b:', max(b))\n",
        "    print('min of b:', min(b))\n",
        "\n",
        "    connections = []\n",
        "    branches = branch_data[:,:2]\n",
        "    for i in range(branches.shape[0]):\n",
        "            connections.append([int(branches[i,0]),int(branches[i,1])])\n",
        "    print('len of connections:', len(connections))\n",
        "\n",
        "    unique_lines, set1, set2 = identify_unique_lines(connections)\n",
        "\n",
        "    B = np.zeros((num_buses, num_buses))\n",
        "    for k, line in unique_lines.items():\n",
        "        row = line[0]-1\n",
        "        col = line[1]-1\n",
        "        B[row, col] = b[k]\n",
        "        B[col, row] = b[k]\n",
        "\n",
        "    PD = bus_data[:,2]/20\n",
        "    print('Total PD:', sum(PD))\n",
        "\n",
        "    return num_buses, num_lines, B, connections, PD"
      ],
      "metadata": {
        "id": "BikI-OcI51au"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataloader.py"
      ],
      "metadata": {
        "id": "_XaV-g1PnHWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_inputs(Ntr, Ntst, Ntr2, scaling, nominal_PD_data, data_path):\n",
        "\n",
        "\n",
        "    forecasts = nominal_PD_data.reshape(1,-1)\n",
        "\n",
        "    lb = np.clip((1-scaling)*forecasts, a_min=0., a_max=None)\n",
        "    ub = (1+scaling)*forecasts\n",
        "\n",
        "    suffix = str(int(scaling*100))+'percent.npy'\n",
        "    print('suffix:', suffix)\n",
        "\n",
        "\n",
        "    train_dataset = np.random.uniform(lb, ub, (Ntr, N))\n",
        "    test_dataset = np.random.uniform(lb, ub, (Ntst, N))\n",
        "\n",
        "    # Generate dataset for pretrain\n",
        "    random_rows = np.random.choice(Ntr, Ntr2)\n",
        "    pretrain_dataset = train_dataset[random_rows,:]\n",
        "    print('pretrain_dataset:', pretrain_dataset.shape)\n",
        "\n",
        "    np.save(data_path+'train_set.npy', train_dataset)\n",
        "    np.save(data_path+'test_set.npy', test_dataset)\n",
        "    np.save(data_path+'pretrain_set.npy', pretrain_dataset)\n",
        "\n",
        "    return train_dataset, test_dataset, pretrain_dataset\n"
      ],
      "metadata": {
        "id": "vmth23da519X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## benchmarks.py"
      ],
      "metadata": {
        "id": "FjLzC7u1nQS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def saa_solver(forecast, num_sce):\n",
        "    '''\n",
        "    Given a single instance of forecast, solve the SAA version\n",
        "    of the true problem.\n",
        "    This is called cp policy in our work.\n",
        "    '''\n",
        "\n",
        "    M = num_sce\n",
        "\n",
        "    x = cp.Variable((N, 1))\n",
        "    theta = cp.Variable((N-1,M))\n",
        "    y = cp.Variable((N, M))\n",
        "\n",
        "    constraints_list = []\n",
        "\n",
        "    for m in range(M):\n",
        "\n",
        "        if Pe == 'uniform':\n",
        "            lb = np.clip((1-ratio)*forecast, a_min=0., a_max=None)\n",
        "            ub = (1+ratio)*forecast\n",
        "            rt_scenario = np.random.uniform(lb, ub, (forecast.shape[0], forecast.shape[1]))\n",
        "\n",
        "        else:\n",
        "\n",
        "            rt_scenario = forecast + sigma*np.random.randn(N, 1) \n",
        "        \n",
        "        d_tilde = rt_scenario - x\n",
        "\n",
        "        constraints_list.append( y[:,m:m+1] == Yrr@theta[:,m:m+1] + d_tilde )\n",
        "        \n",
        "    constraints_list.append( Arr@theta <= Fmax*np.ones((num_lines, M)))\n",
        "    constraints_list.append( Arr@theta >= -Fmax*np.ones((num_lines, M)))\n",
        "\n",
        "    constraints_list.append( theta<=np.pi)\n",
        "    constraints_list.append( theta>=-np.pi)\n",
        "    constraints_list.append( x>=0)\n",
        "\n",
        "    Q_vals = []\n",
        "    for m in range(M):\n",
        "\n",
        "        Q_vals.append(cp.pos(y[:,m:m+1].T)@linear_cost_Coeff)\n",
        "\n",
        "    Q_pred = cp.sum(Q_vals)/M\n",
        "    prob = cp.Problem(cp.Minimize( da_cost_Coeff.T@x + Q_pred ), constraints_list)\n",
        "   \n",
        "    prob.solve(verbose = False, solver = cp.ECOS) ## Works!\n",
        "\n",
        "    return x.value.T, prob.value\n",
        "\n",
        "\n",
        "def ap_solver(forecast, num_sce):\n",
        "    '''\n",
        "    Given a single instance of forecast, solve the SAA version\n",
        "    of the true problem, but with theta approximated by affine policy.\n",
        "    This is called ap policy in our work.\n",
        "    '''\n",
        "\n",
        "    M = num_sce\n",
        "\n",
        "    c = da_cost_Coeff\n",
        "    q = linear_cost_Coeff\n",
        "    B = -Yrr\n",
        "    F = Arr\n",
        "    rho_lb = 10.\n",
        "    rho_ub = 10.\n",
        "\n",
        "    \n",
        "    Pi = cp.Variable((N-1,N))\n",
        "    beta = cp.Variable((N-1,1))\n",
        "\n",
        "    x = cp.Variable((N, 1))\n",
        "\n",
        "\n",
        "    constraints_list = []\n",
        "\n",
        "    Q_vals = []\n",
        "\n",
        "    for m in range(M):\n",
        "        # for each scenario of noise\n",
        "        if Pe == 'uniform':\n",
        "            lb = np.clip((1-ratio)*forecast, a_min=0., a_max=None)\n",
        "            ub = (1+ratio)*forecast\n",
        "            rt_scenario = np.random.uniform(lb, ub, (forecast.shape[0], forecast.shape[1]))\n",
        "\n",
        "        else:\n",
        "\n",
        "            rt_scenario = forecast + sigma*np.random.randn(N, 1) \n",
        "\n",
        "        noise_sce =  rt_scenario - forecast\n",
        "        net_d = rt_scenario - x\n",
        "\n",
        "        theta_pred = Pi@noise_sce + beta\n",
        "        y_pred = net_d - B@theta_pred\n",
        "\n",
        "        p_f_lb = rho_lb*cp.norm( cp.pos(-Fmax*np.ones((num_lines, 1)) - F@theta_pred) )\n",
        "        p_f_ub = rho_ub*cp.norm( cp.pos(F@theta_pred -Fmax*np.ones((num_lines, 1))) )\n",
        "\n",
        "        p_v_lb  = rho_lb*cp.norm( cp.pos(-np.pi*np.ones((N-1, 1)) - theta_pred) )\n",
        "        p_v_ub  = rho_ub*cp.norm( cp.pos(theta_pred - np.pi*np.ones((N-1, 1))) )\n",
        "\n",
        "        Q_val = q.T@cp.pos(y_pred) + p_f_lb + p_f_ub + p_v_lb + p_v_ub\n",
        "\n",
        "        Q_vals.append(Q_val)\n",
        "\n",
        "    constraints_list.append( x>=0)\n",
        "\n",
        "\n",
        "    Q_pred = cp.sum(Q_vals)/M\n",
        "\n",
        "    prob = cp.Problem(cp.Minimize( c.T@x + Q_pred ), constraints_list)\n",
        "\n",
        "    prob.solve(verbose = False, solver = cp.ECOS) ## Works!  \n",
        "\n",
        "    assert prob.status not in [\"infeasible\", \"unbounded\"], \"Problem is not feasible or unbounded.\"\n",
        "\n",
        "\n",
        "    return x.value.T, prob.value\n",
        "\n",
        "\n",
        "def solver_outer_loop(forecasts, num_sce, solver_name):\n",
        "    '''\n",
        "    Given a batch of instances and solve rld problem for each instance.\n",
        "    We only care about x for now.\n",
        "    Can return computation time for each instance at the same time (in minutes)\n",
        "    '''\n",
        "\n",
        "    num_points = forecasts.shape[0]\n",
        "\n",
        "    x_vals = []\n",
        "    total_cost_vals = []\n",
        "    times = []\n",
        "\n",
        "\n",
        "    for it in range(num_points):\n",
        "\n",
        "        forecast = forecasts[it,:].reshape(-1,1)\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        if solver_name == 'saa':\n",
        "            x_value, prob_value = saa_solver(forecast, num_sce)\n",
        "\n",
        "        elif solver_name == 'ap':\n",
        "            x_value, prob_value = ap_solver(forecast, num_sce)\n",
        "\n",
        "        else:\n",
        "            print('No such solver exists!')\n",
        "\n",
        "        end_time = time.time()\n",
        "        times.append((end_time-start_time)/60)\n",
        "\n",
        "        x_vals.append(x_value.flatten())\n",
        "        total_cost_vals.append(prob_value)\n",
        "\n",
        "        # print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
        "        \n",
        "    x_vals = np.array(x_vals)\n",
        "    total_cost_vals = np.array(total_cost_vals).reshape(-1,1)\n",
        "    times = np.array(times)\n",
        "\n",
        "    return x_vals, total_cost_vals, times\n",
        "\n",
        "\n",
        "def evaluate_Q(forecast, x, num_sce):\n",
        "    '''\n",
        "    Given a single instance and the initial dispatch x, and evaluate the quality\n",
        "    of x.\n",
        "    This function can be applied to the decision obtained from any policy.\n",
        "    '''\n",
        "\n",
        "    M = num_sce\n",
        "\n",
        "    theta = cp.Variable((N-1,M))\n",
        "    y = cp.Variable((N, M))\n",
        "\n",
        "    constraints_list = []\n",
        "\n",
        "    for m in range(M):\n",
        "        if Pe == 'uniform':\n",
        "            lb = np.clip((1-ratio)*forecast, a_min=0., a_max=None)\n",
        "            ub = (1+ratio)*forecast\n",
        "            rt_scenario = np.random.uniform(lb, ub, (forecast.shape[0], forecast.shape[1]))\n",
        "\n",
        "        else:\n",
        "\n",
        "            rt_scenario = forecast + sigma*np.random.randn(N, 1) \n",
        "        \n",
        "        net_d = rt_scenario - x\n",
        "\n",
        "        constraints_list.append( y[:,m:m+1] == Yrr@theta[:,m:m+1] + net_d )\n",
        "        \n",
        "    constraints_list.append( Arr@theta <= Fmax*np.ones((num_lines, M)))\n",
        "    constraints_list.append( Arr@theta >= -Fmax*np.ones((num_lines, M)))\n",
        "\n",
        "    constraints_list.append( theta<=np.pi)\n",
        "    constraints_list.append( theta>=-np.pi)\n",
        "\n",
        "    Q_vals = []\n",
        "    for m in range(M):\n",
        "\n",
        "        Q_vals.append(cp.pos(y[:,m:m+1].T)@linear_cost_Coeff)\n",
        "\n",
        "    Q_pred = cp.sum(Q_vals)/M\n",
        "\n",
        "    prob = cp.Problem(cp.Minimize( Q_pred ), constraints_list)\n",
        "\n",
        "    prob.solve(verbose = False, solver = cp.ECOS) ## Works!   \n",
        "\n",
        "    return prob.value, np.sum(y.value, axis=1)/M\n",
        "\n",
        "\n",
        "def evaluate_outer_loop(forecasts, x_pred, num_sce):\n",
        "    '''\n",
        "    Given a batch of instances and the associated predictions of first stage decision,\n",
        "    and evaluate these initial dispatch decisions for each instance.\n",
        "\n",
        "    '''\n",
        "\n",
        "    num_points = forecasts.shape[0]\n",
        "\n",
        "    Q_pred = []\n",
        "    y_pred = []\n",
        "\n",
        "    for i in range(num_points):\n",
        "        forecast = forecasts[i,:].reshape(-1,1)\n",
        "        init_dispatch = x_pred[i,:].reshape(-1,1)\n",
        "\n",
        "        Q_value, avg_y_value = evaluate_Q(forecast, init_dispatch, num_sce)\n",
        "\n",
        "        Q_pred.append(Q_value)\n",
        "        y_pred.append(avg_y_value.flatten())\n",
        "        \n",
        "\n",
        "    Q_pred = np.array(Q_pred).reshape(-1,1)\n",
        "    y_pred = np.array(y_pred)\n",
        "    \n",
        "\n",
        "    total_cost_pred = x_pred@da_cost_Coeff + Q_pred\n",
        "\n",
        "    return total_cost_pred, Q_pred, y_pred\n"
      ],
      "metadata": {
        "id": "jKB5S8_UnUyV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main.py"
      ],
      "metadata": {
        "id": "GA5iXdh_6f71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define params_path, saved_path, data_path, and model_path here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6FzjzTKm69Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_paths():\n",
        "    params_path = root_path+'118bus/params/'\n",
        "    data_path = root_path+'118bus/rld_simulations/data/'\n",
        "    saved_path = root_path+'118bus/simulations/results/'\n",
        "    model_path = root_path+'118bus/rld_simulations/saved_models/'\n",
        "\n",
        "\n",
        "    isExist = os.path.exists(data_path)\n",
        "    if not isExist:\n",
        "        # Create a new directory if it does not exist \n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    \n",
        "    isExist = os.path.exists(saved_path)\n",
        "    if not isExist:\n",
        "        # Create a new directory if it does not exist \n",
        "        os.makedirs(saved_path)\n",
        "\n",
        "    return params_path, data_path, saved_path, model_path"
      ],
      "metadata": {
        "id": "1d257ZpX7C3j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import 118bus system"
      ],
      "metadata": {
        "id": "KTuqyFru6lsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_saved_models(model_path):\n",
        "\n",
        "    action_net = ActionNet(N, 256, N)\n",
        "    action_net.load_state_dict(torch.load(model_path+'trained_action_net.pt'))\n",
        "\n",
        "    reward_net = RewardNet(N, 256, N-1)\n",
        "    reward_net.load_state_dict(torch.load(model_path+'trained_reward_net.pt'))\n",
        "\n",
        "    return action_net, reward_net\n",
        "\n",
        "\n",
        "def load_pretrain_models(model_path):\n",
        "\n",
        "    action_net = ActionNet(N, 256, N)\n",
        "    action_net.load_state_dict(torch.load(model_path+'trained_action_net_sl.pt'))\n",
        "\n",
        "    reward_net = RewardNet(N, 256, N-1)\n",
        "    reward_net.load_state_dict(torch.load(model_path+'trained_reward_net_sl.pt'))\n",
        "\n",
        "    return action_net, reward_net\n",
        "\n",
        "\n",
        "def load_cost_coeff(params_path):    \n",
        "    quad_cost_coeff = np.load(params_path+'quad_cost_coeff.npy')\n",
        "    linear_cost_coeff = np.load(params_path+'linear_cost_coeff.npy')\n",
        "    da_cost_coeff = np.load(params_path+'da_cost_coeff.npy')\n",
        "\n",
        "    return quad_cost_coeff, linear_cost_coeff, da_cost_coeff\n",
        "\n",
        "\n",
        "params_path, data_path, saved_path, model_path = get_paths()\n",
        "\n",
        "\n",
        "num_buses, num_lines, B, connections, PD_data = import_118bus(params_path)\n",
        "\n",
        "N = num_buses\n",
        "L = num_lines \n",
        "\n",
        "Yrr = get_Y(num_buses, B)\n",
        "Arr = get_A(num_buses, num_lines, B, connections)\n",
        "\n",
        "# Define feasibility set for gauge mapping\n",
        "G = np.block([\n",
        "              [Arr],\n",
        "              [-Arr],\n",
        "              [np.eye(N-1)],\n",
        "              [-np.eye(N-1)]\n",
        "])\n",
        "print('G shape:', G.shape)\n",
        "\n",
        "\n",
        "Fmax = 1.\n",
        "\n",
        "\n",
        "# If forecast error follows gaussian distribution with zero mean and std sigma\n",
        "Pe = 'gaussian'\n",
        "sigma = 1.\n",
        "\n",
        "# If forecast error follows interval bounded distribution-uniform then ratio is 5%\n",
        "# Pe = 'uniform'\n",
        "ratio = 0.05\n",
        "\n",
        "# Load cost coefficients\n",
        "quad_cost_coeff, linear_cost_coeff, da_cost_coeff = load_cost_coeff(params_path) \n",
        "\n",
        "quad_cost_Coeff = np.diag(quad_cost_coeff)\n",
        "linear_cost_Coeff = linear_cost_coeff.reshape(-1, 1)\n",
        "da_cost_Coeff = da_cost_coeff.reshape(-1,1)\n",
        "\n",
        "print('quad_cost_Coeff shape:', quad_cost_Coeff.shape)\n",
        "print('linear_cost_Coeff shape:', linear_cost_Coeff.shape)\n",
        "print('da_cost_Coeff shape:', da_cost_Coeff.shape)"
      ],
      "metadata": {
        "id": "En0JQv5h6pcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa62a8bf-bf77-46d3-f593-6785b55d8b2e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max of x: 0.411\n",
            "min of x: 0.004\n",
            "max of b: 25.0\n",
            "min of b: 0.24330900243309003\n",
            "len of connections: 186\n",
            "unique_lines length: 179\n",
            "Total PD: 212.10000000000005\n",
            "G shape: (606, 117)\n",
            "quad_cost_Coeff shape: (118, 118)\n",
            "linear_cost_Coeff shape: (118, 1)\n",
            "da_cost_Coeff shape: (118, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load.py"
      ],
      "metadata": {
        "id": "uiIAZCIdQ5Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nominal_PD_data = PD_data.reshape(1,-1)\n",
        "\n",
        "# Load dataset\n",
        "scaling = 0.1\n",
        "Ntr = 50000\n",
        "Ntr2 = 100\n",
        "Ntst = 100\n",
        "\n",
        "# _, test_dataset, pretrain_dataset =generate_inputs(Ntr, Ntst, Ntr2,\n",
        "#                                                     scaling, nominal_PD_data, \n",
        "#                                                     data_path)\n",
        "\n",
        "train_dataset = np.load(data_path+'train_set.npy')\n",
        "test_dataset = np.load(data_path+'test_set.npy')\n",
        "pretrain_dataset = np.load(data_path+'pretrain_set.npy')\n",
        "\n",
        "max_forecast_val = np.max(train_dataset)\n",
        "# Note that the original train_dataset and test_dataset are not normalized\n",
        "TrainSet = TrainDataset(train_dataset/max_forecast_val)\n",
        "TestSet = TrainDataset(test_dataset/max_forecast_val)\n"
      ],
      "metadata": {
        "id": "WMUjPi3xQ8XO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pretrain.py"
      ],
      "metadata": {
        "id": "KMe2BbK0-52U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format:\n",
        "# X_train, X_test: [forecasts, x_true]\n",
        "# X_train2, X_test2:[forecasts-x_true, theta_true, y_true, Q_true]\n",
        "\n",
        "X_train = np.load(data_path+'X_train.npy')\n",
        "X_test = np.load(data_path+'X_test.npy')\n",
        "X_train2 = np.load(data_path+'X_train2.npy')\n",
        "X_test2 = np.load(data_path+'X_test2.npy')"
      ],
      "metadata": {
        "id": "kNalKghD_LIK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pretrain action net"
      ],
      "metadata": {
        "id": "YBerukVn_HHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_action(dataset, num_epochs, batch_size, yplot = 'plain'):\n",
        "\n",
        "    # Set learning rates\n",
        "    lr = 1e-5\n",
        "\n",
        "    # Build models or load models\n",
        "    # action_net, reward_net = load_saved_models()\n",
        "    action_net = ActionNet(N, 256, N)\n",
        "    action_net.apply(init_weights)\n",
        "    \n",
        "    # action_net.load_state_dict(torch.load(saved_path+'trained_action_net_sl.pt'))\n",
        "\n",
        "    # return action_net\n",
        "\n",
        "    # Configure optimizers\n",
        "    # action_optim = optim.Adam(action_net.parameters(), lr=lr, weight_decay=1e-2)\n",
        "    action_optim = optim.Adam(action_net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    loss_criterion = nn.MSELoss()\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    train_hist = {}\n",
        "    train_hist['train_losses'] = []\n",
        "    train_hist['per_epoch_time'] = []\n",
        "    train_hist['total_time'] = []\n",
        "    num_iter = 0\n",
        "\n",
        "    print('Training starts!')\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        for batch in train_loader:\n",
        "\n",
        "            ## zero grad\n",
        "            action_net.zero_grad()\n",
        "\n",
        "            ## get a batch of data\n",
        "            input = batch.float()\n",
        "            load = input[:, :N]\n",
        "            target = input[:, N:]\n",
        "\n",
        "            x_pred = action_net(load)\n",
        "\n",
        "            train_loss = loss_criterion(x_pred, target)\n",
        "\n",
        "            train_loss.backward(retain_graph=True)\n",
        "            action_optim.step()\n",
        "\n",
        "            train_losses.append(train_loss.item())\n",
        "\n",
        "\n",
        "            num_iter += 1\n",
        "\n",
        "            if num_iter % 2000 == 0:\n",
        "                print('[%d/%d], loss: %.5f' % ((epoch + 1), \n",
        "                    (num_iter + 1), \n",
        "                    torch.FloatTensor(train_losses).mean().item()))\n",
        "                    \n",
        "        epoch_end_time = time.time()\n",
        "        per_epoch_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        loss_value = torch.mean(torch.FloatTensor(train_losses)).item()\n",
        "\n",
        "        train_hist['train_losses'].append(round(loss_value, 4))\n",
        "        train_hist['per_epoch_time'].append(per_epoch_time/60)\n",
        "                \n",
        "        if epoch > 0 :\n",
        "            if epoch % 20000 == 0 or epoch == num_epochs-1:\n",
        "\n",
        "                torch.save(action_net.state_dict(), saved_path+'pretrain_action_net.pt')\n",
        "\n",
        "                plot_train_loss(train_hist, epoch, 'pretrain_action_loss.png', yplot)\n",
        "                \n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    train_hist['total_time'].append(total_time)\n",
        "\n",
        "    print(\"Avg per epoch time: %.2f minutes, total %d epochs time: %.2f minutes\" % \\\n",
        "        (torch.mean(torch.FloatTensor(train_hist['per_epoch_time'])), num_epochs, total_time/60))\n",
        "    print(\"Training finish!\")\n",
        "\n",
        "    return action_net\n"
      ],
      "metadata": {
        "id": "caBa-HP4ARPo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pretrain reward net"
      ],
      "metadata": {
        "id": "UxsGHP_h_JTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_reward(dataset, num_epochs, batch_size, yplot = 'plain'):\n",
        "\n",
        "    # Set learning rates\n",
        "    lr = 0.0001\n",
        "\n",
        "    # Build models or load models\n",
        "    # action_net, reward_net = load_saved_models()\n",
        "    reward_net = RewardNet(N, 256, N-1)\n",
        "\n",
        "    # reward_net.load_state_dict(torch.load(saved_path+'trained_reward_net_sl.pt'))\n",
        "\n",
        "    # return reward_net\n",
        "    \n",
        "\n",
        "    # Configure optimizers\n",
        "    reward_optim = optim.Adam(reward_net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    loss_criterion = nn.MSELoss()\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    train_hist = {}\n",
        "    train_hist['train_losses'] = []\n",
        "    train_hist['per_epoch_time'] = []\n",
        "    train_hist['total_time'] = []\n",
        "    num_iter = 0\n",
        "\n",
        "    G_ts = torch.tensor(G).float()\n",
        "    B_ts = torch.tensor(-Yrr).float()\n",
        "    F_ts = torch.tensor(Arr).float()\n",
        "    q_ts = torch.tensor(linear_cost_Coeff).float()\n",
        "    c_ts = torch.tensor(da_cost_Coeff).float()\n",
        "\n",
        "    print('Training starts!')\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        for batch in train_loader:\n",
        "\n",
        "            ## zero grad\n",
        "            reward_net.zero_grad()\n",
        "\n",
        "            ## get a batch of data\n",
        "            input = batch.float()\n",
        "            q_input = input[:, :N]\n",
        "            target = input[:, N:N+N-1]\n",
        "            y_target = input[:, N+N-1:N+N-1+N]\n",
        "            Q_target = input[:, N+N-1+N:]\n",
        "\n",
        "            pred = reward_net(q_input)\n",
        "\n",
        "            # net_d = q_input*max_val\n",
        "            net_d = q_input\n",
        "\n",
        "            h_batch = torch.cat([\n",
        "                                Fmax*torch.ones(L, input.shape[0]),\n",
        "                                Fmax*torch.ones(L, input.shape[0]),\n",
        "                                np.pi*torch.ones(N-1, input.shape[0]),\n",
        "                                np.pi*torch.ones(N-1, input.shape[0])\n",
        "                                ],dim=0)\n",
        "\n",
        "            h_batch_ts = torch.tensor(h_batch).float()\n",
        "            \n",
        "            ctr_ts = torch.zeros((N-1, input.shape[0]))\n",
        "            assert torch.max(G_ts@ctr_ts - h_batch_ts)< 0.0, \"Caution: v0 is not a feasible point.\"\n",
        "\n",
        "            # Use ctr_ts to shift the feasible set Gv<=H\n",
        "            h_shift = h_batch_ts - G_ts@ctr_ts\n",
        "            assert torch.min(h_shift) > 0.0, \"Caution: origin is not an interior point\"\n",
        "\n",
        "            # Use gauge mapping to transform output to feas_output\n",
        "\n",
        "            feas_pred = gauge_map(pred.T, G_ts, h_shift) + ctr_ts\n",
        "\n",
        "            y_pred = -B_ts@feas_pred + net_d.T # (N, batch_size)\n",
        "            Q_pred = F.relu(y_pred.T)@q_ts\n",
        "\n",
        "            train_loss = loss_criterion(feas_pred.T, target)\n",
        "\n",
        "            train_loss.backward(retain_graph=True)\n",
        "            reward_optim.step()\n",
        "\n",
        "            train_losses.append(train_loss.item())\n",
        "\n",
        "            num_iter += 1\n",
        "\n",
        "            if num_iter % 10000 == 0:\n",
        "                print('[%d/%d], loss: %.5f' % ((epoch + 1), \n",
        "                    (num_iter + 1), \n",
        "                    torch.FloatTensor(train_losses).mean().item()))\n",
        "                    \n",
        "        epoch_end_time = time.time()\n",
        "        per_epoch_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        loss_value = torch.mean(torch.FloatTensor(train_losses)).item()\n",
        "\n",
        "        train_hist['train_losses'].append(round(loss_value, 4))\n",
        "        train_hist['per_epoch_time'].append(per_epoch_time/60)\n",
        "                \n",
        "        if epoch > 0 :\n",
        "            if epoch % 100000 == 0 or epoch == num_epochs-1:\n",
        "\n",
        "                torch.save(reward_net.state_dict(), saved_path+'pretrain_reward_net.pt')\n",
        "\n",
        "                plot_train_loss(train_hist, epoch, 'pretrain_reward_loss.png', yplot)\n",
        "                \n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    train_hist['total_time'].append(total_time)\n",
        "\n",
        "    print(\"Avg per epoch time: %.2f minutes, total %d epochs time: %.2f minutes\" % \\\n",
        "        (torch.mean(torch.FloatTensor(train_hist['per_epoch_time'])), num_epochs, total_time/60))\n",
        "    print(\"Training finish!\")\n",
        "\n",
        "    return reward_net\n"
      ],
      "metadata": {
        "id": "WY7LrZ3N_DkW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train.py"
      ],
      "metadata": {
        "id": "4NR7VJ_H-71q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, num_epochs, num_sce, batch_size, yplot = 'plain', use_pretrain='True', use_saved=False):\n",
        "\n",
        "    # Set learning rates\n",
        "    lr = 0.0001\n",
        "\n",
        "    # Build models or load models\n",
        "    if use_pretrain and not use_saved:\n",
        "        action_net, reward_net = load_pretrain_models(model_path)\n",
        "\n",
        "        action_net.train(True)\n",
        "        reward_net.train(True)\n",
        "\n",
        "    elif not use_pretrain and use_saved:\n",
        "        action_net, reward_net = load_saved_models(model_path)\n",
        "\n",
        "        action_net.train(True)\n",
        "        reward_net.train(True)\n",
        "\n",
        "    else:\n",
        "        action_net = ActionNet(N, 256, N)\n",
        "        reward_net = RewardNet(N, 256, N-1)\n",
        "\n",
        "    # Configure optimizers\n",
        "    action_optim = optim.Adam(action_net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    reward_optim = optim.Adam(reward_net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "\n",
        "    # Constant Parameters\n",
        "    G_ts = torch.tensor(G).float()\n",
        "    B_ts = torch.tensor(-Yrr).float()\n",
        "    q_ts = torch.tensor(linear_cost_Coeff).float()\n",
        "    c_ts = torch.tensor(da_cost_Coeff).float()\n",
        "    max_forecast_ts = torch.tensor(max_forecast_val).float()\n",
        "\n",
        "    h_batch = torch.cat([\n",
        "                        Fmax*torch.ones(L, batch_size),\n",
        "                        Fmax*torch.ones(L, batch_size),\n",
        "                        np.pi*torch.ones(N-1, batch_size),\n",
        "                        np.pi*torch.ones(N-1, batch_size)\n",
        "                        ],dim=0)\n",
        "\n",
        "    h_batch_ts = torch.tensor(h_batch).float()\n",
        "\n",
        "    ctr_ts = torch.zeros((num_buses-1, batch_size))\n",
        "    assert torch.max(G_ts@ctr_ts - h_batch_ts)< 0.0, \"Caution: v0 is not a feasible point.\"\n",
        "\n",
        "    # Use ctr_ts to shift the feasible set Gv<=H\n",
        "    h_shift = h_batch_ts - G_ts@ctr_ts\n",
        "    assert torch.min(h_shift) > 0.0, \"Caution: origin is not an interior point\"\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    train_hist = {}\n",
        "    train_hist['train_losses'] = []\n",
        "    train_hist['per_epoch_time'] = []\n",
        "    train_hist['total_time'] = []\n",
        "    num_iter = 0\n",
        "\n",
        "    print('Training starts!')\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_losses = []\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        for batch in train_loader:\n",
        "\n",
        "             ## zero grad\n",
        "            action_net.zero_grad()\n",
        "            reward_net.zero_grad()\n",
        "\n",
        "            ## get a batch of data\n",
        "            input = batch.float()\n",
        "            x = action_net(input)# this input has been scaled, so x is treated as scaled\n",
        "\n",
        "            Q_pred = 0.\n",
        "            for m in range(num_sce):          \n",
        "                if Pe == 'uniform':  \n",
        "                    input_val = input.detach().numpy()\n",
        "                    lb = np.clip((1-ratio)*input_val, a_min=0., a_max=None)\n",
        "                    ub = (1+ratio)*input_val\n",
        "                    rt_scenario = np.random.uniform(lb, ub, (input_val.shape[0], input_val.shape[1])) \n",
        "                    rt_scenario = torch.tensor(rt_scenario).float()*max_forecast_ts\n",
        "\n",
        "                else:\n",
        "                    noise_sce = sigma*torch.randn_like(input)\n",
        "                    rt_scenario = input*max_forecast_ts + noise_sce\n",
        "\n",
        "                q_input = rt_scenario - x*max_forecast_ts\n",
        "\n",
        "                pred = reward_net(q_input)\n",
        "\n",
        "                net_d = q_input # (N, bsize)\n",
        "\n",
        "                # Use gauge mapping to transform output to feas_output\n",
        "                feas_pred = gauge_map(pred.T, G_ts, h_shift) + ctr_ts\n",
        "\n",
        "                # Construct cost function\n",
        "                y_pred = -B_ts@feas_pred + net_d.T # (N, batch_size)\n",
        "                # quad_cost = torch.diagonal(y.T@C1_ts@y)\n",
        "                linear_cost = F.relu(y_pred.T)@q_ts\n",
        "                # rt_cost += quad_cost + linear_cost.flatten()\n",
        "                Q_pred += linear_cost.flatten()\n",
        "\n",
        "            Q_pred = Q_pred/num_sce\n",
        "\n",
        "            x_pred = x*max_forecast_ts\n",
        "\n",
        "            da_cost = x_pred@c_ts\n",
        "            total_cost = Q_pred + da_cost.flatten()\n",
        "\n",
        "            train_loss = torch.mean(total_cost) \n",
        "\n",
        "            train_loss.backward(retain_graph=True)\n",
        "            action_optim.step()\n",
        "            reward_optim.step()\n",
        "\n",
        "            train_losses.append(train_loss.item())\n",
        "\n",
        "\n",
        "            num_iter += 1\n",
        "\n",
        "            if num_iter % 5000 == 0:\n",
        "                print('[%d/%d], loss: %.5f' % ((epoch + 1), \n",
        "                    (num_iter + 1), \n",
        "                    torch.FloatTensor(train_losses).mean().item()))\n",
        "                \n",
        "        epoch_end_time = time.time()\n",
        "        per_epoch_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        loss_value = torch.mean(torch.FloatTensor(train_losses)).item()\n",
        "\n",
        "        train_hist['train_losses'].append(round(loss_value, 4))\n",
        "        train_hist['per_epoch_time'].append(per_epoch_time/60)\n",
        "                \n",
        "        if epoch > 0 :\n",
        "            if epoch % 200 == 0 or epoch == num_epochs-1:\n",
        "\n",
        "                torch.save(action_net.state_dict(), saved_path+'trained_action_net.pt')\n",
        "\n",
        "                torch.save(reward_net.state_dict(), saved_path+'trained_reward_net.pt')\n",
        "\n",
        "                plot_train_loss(train_hist, epoch, 'train_loss.png', yplot)\n",
        "                \n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    train_hist['total_time'].append(total_time)\n",
        "\n",
        "    print(\"Avg per epoch time: %.2f minutes, total %d epochs time: %.2f minutes\" % \\\n",
        "        (torch.mean(torch.FloatTensor(train_hist['per_epoch_time'])), num_epochs, total_time/60))\n",
        "    print(\"Training finish!\")\n",
        "\n",
        "\n",
        "    return action_net, reward_net\n"
      ],
      "metadata": {
        "id": "P8NVjBgEDPPZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# call.py"
      ],
      "metadata": {
        "id": "Y76YA5fpE5u_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrain_action_net = train_action(TrainDataset(X_train), num_epochs=10, \n",
        "#                                    batch_size=1, yplot = 'log10')"
      ],
      "metadata": {
        "id": "rhwdeOdOE5LP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrain_reward_net = train_action(TrainDataset(X_train2), num_epochs=10, \n",
        "#                                    batch_size=1, yplot = 'log10')"
      ],
      "metadata": {
        "id": "bN0P-OQLFMV2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_action_net, trained_reward_net = train(TrainSet, num_epochs=1400, \n",
        "                                               num_sce=10, batch_size=100, \n",
        "                                               yplot = 'plain')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBamtzS2FOMC",
        "outputId": "09089c6c-ce6d-4a70-8261-093f7c37ebee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training starts!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# eval.py\n"
      ],
      "metadata": {
        "id": "jRt5BKwz8sRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_ap = 10\n",
        "N_cp = 500\n",
        "N_eval = 500"
      ],
      "metadata": {
        "id": "bkHMRQtG-los"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proposed neural policy"
      ],
      "metadata": {
        "id": "ermujEmA-jGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load neural solutions\n",
        "# x_nu = np.load(data_path+'x_nu.npy')\n",
        "\n",
        "# otherwise\n",
        "# Load saved models from model_path or saved_path\n",
        "action_net, _ = load_saved_models(model_path)\n",
        "\n",
        "action_net.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    input = torch.tensor(test_dataset/max_forecast_val).float()\n",
        "    start_time = time.time()\n",
        "\n",
        "    output = action_net(input)\n",
        "\n",
        "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
        "\n",
        "    output_val = output.detach().numpy()\n",
        "\n",
        "    x_nu = output_val*max_forecast_val"
      ],
      "metadata": {
        "id": "DZG-clgZ-3GR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd01431-2127-4244-fd1a-1fa9c2b4115d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 2.9969215393066407e-05 minutes ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate using evaluate_outer_loop(forecasts, x_pred)\n",
        "total_cost_nu, Q_nu, y_nu = evaluate_outer_loop(test_dataset, x_nu, num_sce=N_eval)\n",
        "\n",
        "np.save(saved_path+'x_nu.npy', x_nu)\n",
        "np.save(saved_path+'total_cost_nu.npy', total_cost_nu)\n",
        "np.save(saved_path+'Q_nu.npy', Q_nu)\n",
        "np.save(saved_path+'y_nu.npy', y_nu)"
      ],
      "metadata": {
        "id": "jWDjNfv2-KOP"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# plotting"
      ],
      "metadata": {
        "id": "r8Qfgv65GQ5h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JurNEAtVGQMw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}